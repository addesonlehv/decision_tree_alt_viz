# Introduction

Conventional decision trees visualizations -- complete with nodes, branches, and leaves -- are an invaluable tool in statistical modeling. They help us understand regression "splits" (that is, how data is partitioned based on independent variables) and make predictions about new, unseen data. In other words, decision trees are an effective way to take a data point and follow a sequence of decision rules (thus creating the "tree" shape) to ultimately arrive at a predicted value for the outcome variable. But decision tree plots aren’t the only way that we could, or should, visualize how a model makes regression decisions. 

Indeed, the classic decision tree has some interpretability challenges inherent in the node and branch structure. Particularly when the decision trees are wide and deep, these visualizations can be tricky to follow and (ultimately) make predictions off of. When we have two (non-binary) predictor variables, we can create an alternative visualization that basically maps the individual decision splits directly onto a predictor space with two variables -- one predictor on the x-axis and another on the y-axis -- so that we can see how and where the decision tree partitions the data: 

![Decision tree alternative visualization by Prof. Robbins](image.png)

More specifically, the space is divided into rectangular regions according to the splits made by the tree -- with each region corresponding to a terminal node (or leaf) of a decision tree and is labeled with the predicted value assigned to that region. The vertical and horizontal lines show the values at which the model splits the data.


As we can see, the practical utility of this visual is that if we have the value of two predictors (in this case years playing baseball and number of hits), we can simply find what rectangular region that specific combination of predictors is in. The label of the rectangular region (in this example, the log salary of the player) is our prediction. The benefit of this visualization method is that it provides an immediate and (in my opinion, at least) more intuitive understanding of how the regression model actually segments the predictor space, making it more straightforward to apply the model to make predictions about new data, without having to trace through a complex tree.

However, the reason that this sample visualization was hand drawn (rather than done digitally in R) is because, according to Professor Robbins, there really isn’t a simple, straightforward way to make these visualizations in R. My final project is an attempt to address that issue. To that end, I’ll be motivated by the following research question: how can we use R to design an alternative regression model visualization that takes information typically used to build a traditional decision tree and instead use it to automatically create a visualization that maps decision splits directly onto a predictor space? 

The majority of this project will focus on programming the function in R that digitally creates this visualization. To do this, I intend to use ggplot functions as well as the model results that come from the rpart function (the same function that helps us to create the traditional decision trees). If this part is successful, I’ll potentially attempt to extend this project further by creating my own package in R which contains my visualization function (though, again, this is more aspirational). The final paper will include an in-depth description of the research question at hand and my method for creating the visualization function (that is, specifically addressing how I wrote the R code). For the results section, I’ll provide the R code for the function and illustrate its usage with sample data. Finally, in the conclusion, I’ll include a more general discussion of the alternative visualization method and reflections on the process of writing my function that creates it.