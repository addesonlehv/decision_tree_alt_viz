# Introduction

Conventional decision tree visualizations -- complete with nodes, branches, and leaves -- are an invaluable tool in statistical modeling. They help us understand regression "splits" (that is, how data is partitioned based on independent variables) and make predictions about new, unseen data. In other words, decision trees are an effective way to take a data point and follow a sequence of decision rules (thus creating the "tree" shape) to ultimately arrive at a predicted value for the outcome variable. But decision tree plots aren’t the only way that we could, or should, visualize how a model makes regression decisions. 

Indeed, the classic decision tree has some interpretability challenges inherent in the node and branch structure. Particularly when the decision trees are wide and deep, these visualizations can be tricky to follow and (ultimately) make predictions off of. When we have two numerical predictor variables, we can create an alternative visualization that basically maps the individual decision splits directly onto a predictor space with two variables -- one predictor on the x-axis and another on the y-axis -- so that we can see how and where the decision tree partitions the data:

![Alternative visualization of a decision tree by Professor Robbins](image.png)

As we can see, the space is divided into rectangular regions according to the splits made by the tree -- with each region corresponding to a leaf node of a decision tree and is labeled with the predicted value assigned to that region. The vertical and horizontal lines show the values at which the model splits the data. Ultimately, the advantage of this visualization method is that it provides an immediate and (in my opinion, at least) more intuitive understanding of how the regression model actually partitions the predictor space, making it more straightforward to apply the model to make predictions about new data, without necessarily having to trace through a complex tree. This alternative visualization method also allows us to make other (cool) informative graphs that wouldn't be possible with the conventional decision tree: for example, we could plot individual data points from the original dataset directly onto the predictor space created by our alternative visualization, making it easy to compare each observed data point to the predicted value assigned to its region.

However, the reason that this sample visualization was hand drawn (rather than done digitally in R) is because there isn’t really a simple, readily available way to make these visualizations automatically in R. This project is a pilot study attempt to address that issue. To that end, the project will be motivated by the following question: **to what extent can we use `R` to design a function that creates an alternative visualization for decision trees with one or two numerical predictors -- specifically, one that takes the statistical information typically used to build a conventional decision tree and instead maps the decision splits directly onto a predictor space (as shown in our example)?**

The next four sections of this website dive deeper into this question. The subsequent section describes the method and approach for creating this function, walking through step-by-step how the function in `R` works. Then, we'll take a look at how to use the function and illustrate its usage using sample data from a variety of datasets. Finally, the final two sections will outline future directions for this pilot project and provide some concluding thoughts.