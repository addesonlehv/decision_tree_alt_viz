[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Tree Alternative Visualization",
    "section": "",
    "text": "1 Introduction\nConventional decision tree visualizations – complete with nodes, branches, and leaves – are an invaluable tool in statistical modeling. They help us understand regression “splits” (that is, how data is partitioned based on independent variables) and make predictions about new, unseen data. In other words, decision trees are an effective way to take a data point and follow a sequence of decision rules (thus creating the “tree” shape) to ultimately arrive at a predicted value for the outcome variable. But decision tree plots aren’t the only way that we could, or should, visualize how a model makes regression decisions.\nIndeed, the classic decision tree has some interpretability challenges inherent in the node and branch structure. Particularly when the decision trees are wide and deep, these visualizations can be tricky to follow and (ultimately) make predictions off of. When we have two numerical predictor variables, we can create an alternative visualization that basically maps the individual decision splits directly onto a predictor space with two variables – one predictor on the x-axis and another on the y-axis – so that we can see how and where the decision tree partitions the data:\n\n\n\nAlternative visualization of a decision tree by Professor Robbins\n\n\nAs we can see, the space is divided into rectangular regions according to the splits made by the tree – with each region corresponding to a leaf node of a decision tree and is labeled with the predicted value assigned to that region. The vertical and horizontal lines show the values at which the model splits the data. Ultimately, the advantage of this visualization method is that it provides an immediate and (in my opinion, at least) more intuitive understanding of how the regression model actually partitions the predictor space, making it more straightforward to apply the model to make predictions about new data, without necessarily having to trace through a complex tree. This alternative visualization method also allows us to make other (cool) informative graphs that wouldn’t be possible with the conventional decision tree: for example, we could plot individual data points from the original dataset directly onto the predictor space created by our alternative visualization, making it easy to compare each observed data point to the predicted value assigned to its region.\nHowever, the reason that this sample visualization was hand drawn (rather than done digitally in R) is because there isn’t really a simple, readily available way to make these visualizations automatically in R. This project is a pilot study attempt to address that issue. To that end, the project will be motivated by the following question: to what extent can we use R to design a function that creates an alternative visualization for decision trees with one or two numerical predictors – specifically, one that takes the statistical information typically used to build a conventional decision tree and instead maps the decision splits directly onto a predictor space (as shown in our example)?\nThe next four sections of this website dive deeper into this question. The subsequent section describes the method and approach for creating this function, walking through step-by-step how the function in R works. Then, we’ll take a look at how to use the function and illustrate its usage using sample data from a variety of datasets. Finally, the final two sections will outline future directions for this pilot project and provide some concluding thoughts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods and Function",
    "section": "",
    "text": "2.1 Coding Approach\nPresumably, there are many possible ways to go about creating this alternative visualization function. So where should we start? When looking at the decision splits in the sample alternative visualization, there seemed to really be two different ways we could go about recreating this graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods and Function</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-approach",
    "href": "methods.html#coding-approach",
    "title": "2  Methods and Function",
    "section": "",
    "text": "2.1.1 Approach 1\nFirst, we could think about the splits as being created by horizontal and vertical lines with each line representing a decision split. If we were careful to truncate each line so that it only spans the region where the corresponding split condition applies, we can use these boundaries to define predictions regions, which we could then label with the prediction value. This method seemed somewhat challenging to implement in R, however – especially considering that we’d need to be able to effectively truncate each line.\n\n\n2.1.2 Approach 2\nThe second approach, then, is to think about each of the prediction regions as rectangles. Each of these rectangles would have coordinates defined by the minimum and maximum values of the predictor variables that satisfy the split conditions leading to a particular leaf. For example, if we look at the sample alternative visualization with the prediction 5.3, we can see that the minimum x-value that would satisfy the prediction is 4 years and the maximum value is 5 years. Similarly, the minimum y-value that would satisfy the prediction is 0 hits and the maximum value is 125 hits. This, in turn, gives us coordinates of a rectangle – (4, 0), (4, 125), (5, 0), and (5, 125) – that represents the prediction region. In other words, by tracing the sequence of decision rules for each leaf node, we can derive the exact bounds of the region it occupies in the predictor space – and subsequently use those bounds to draw the rectangle and assign its predicted value.\n That is, we begin with the individual leaf nodes and their corresponding predicted values. Then, after examining the split conditions that lead to each leaf, we determine the bounds of each predictor variable we can use to define the coordinates of the rectangles used in the graph. This contrasts with a more “top-down” approach, where one might start with the decision splits and work forward to identify the resulting prediction regions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods and Function</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-the-function",
    "href": "methods.html#coding-the-function",
    "title": "2  Methods and Function",
    "section": "2.2 Coding the Function",
    "text": "2.2 Coding the Function\nWith this general approach in mind, we ultimately know what goal we are aiming for: some sort of geom_rect (rectangle) function that makes a bunch of rectangles based on the coordinates set by the decision split bounds. Thus, we first need to somehow extract each prediction region’s decision split bounds and each region’s corresponding prediction. Before we begin though, we need to do a little bit of “housekeeping” to set up the function and get the information that we need.\n\n2.2.1 Housekeeping\nThe first step is to simply set up the function. We will call our function alt_viz (short for alternative visualization) and it will take two inputs. The first input, mod, is a regression model that is created using the rpart function in the rpart package – thus making the rpart package a requirement for using the function. The second input, type, is a numeric value that corresponds to 1, 2, or 3. We’ll talk about this more later, but it allows the user to control what type of output they receive from running the function. This is the first line of code of our function:\n\n# calling the function\nalt_viz &lt;- function(mod, type = 1) {\n\nNow, we need to figure out how to extract the necessary information from the model that is created using the rpart function. We know that the model created contains the information needed to build the classic decision tree – since the rpart.plot function, which makes these conventional visualizations, also uses this model. This is where we need to be a little creative. A little bit of research tells us that there’s a function called path.rpart, which retrieves a sequence of split conditions – that is, the decision rules – that lead from the root of the tree to each specified node. For example, using the same baseball data that was used in the sample alternative visualization, the result of the path.rpart function for one (leaf) node looks like this:\n\nnode number:27 root  Years&gt;=4.5  Hits&lt; 117.5 Years&gt;=6.5 Hits&gt;=50.5\n\nThis sequence should tell us everything that we need to know about the split decisions for this particular (leaf) node: after the root, if the number of years a player has been playing baseball in the Major League is greater than or equal to 4.5 years, and they have fewer than 117.5 hits, and have been playing for at least 6.5 years, and have 50.5 or more hits, then they fall into this specific prediction region. This mean that we can create the bounds of the rectangular prediction region for this node – and other nodes – if we do the following:\n\nTurn the sequencing rules into a vector that contains the minimum and maximum values for the Hits and Years predictors. For example, we know that the minimum value for Hits in this prediction region is 50.5 and the maximum value is 117.5. Some additional points to consider:\n\nIf there are more than one lower or upper bound conditions, like \\(Years \\geq 4.5\\) and \\(Years \\geq 6.5\\), then we need to keep the more restrictive condition – \\(Years \\geq 6.5\\), in this instance – since it defines a narrower region.\nIf there is no explicitly specified inequality, like \\(Years &lt; {}...{}\\), then we should assume that there is no upper or lower constraint in that direction and, thus, should fall back on the reasonable default of the maximum or minimum value of the dataset.\nIf we are interested in generalizability to new datasets (which we are), then we need to design the code logic such that it doesn’t rely on any specific dataset predictor names. Rather, it should flexibly adapt to new trees and new data.\n\nIdentify only the leaf nodes and have the path.rpart function return the sequence of split conditions that lead to each one.\nApply a function that applies what the first bullet point suggests to all of our leaf nodes. This will create a list for each leaf node and its relevant rectangle boundaries for their prediction regions.\n\nWe can take these principles and turn them into code. But first, just a little bit more housekeeping.\n\n\n2.2.2 More Housekeeping\nAs we saw, we need to design our code so that it’s not reliant on one specific dataset. This means writing it in a way that dynamically adapts to different variable names, ranges, and structures without hard-coded assumptions. The next couple of lines of the function executes on this:\n\n# extracting model specific information\n  names &lt;- unique(mod$frame$var)\n  names &lt;- names[names != \"&lt;leaf&gt;\"]\n\n# account for the case where only one predictor is important\n  if (length(names) == 1) {\n    xname &lt;- names[1]\n    yname &lt;- NULL\n  } else {\n    yname &lt;- names[1]\n    xname &lt;- names[2]\n  }\n\n# find the name of the original dataset\n  modelname &lt;- mod$call$data\n\nThe function first extracts the list of names of the predictors (like Years and Hits) by looking at what variable names are produced. We also need to remove the &lt;leaf&gt; name from this list as it’s not a predictor.\nThen, we want to account for the fact that even if we include two predictors in our regression model, perhaps only one of them is actually important for splits. The if...else statement assigns the vertical predictor value (yname) to NULL. We’ll deal with this part more later.\nFinally, we need to extract the name of the original dataset that the model used. As we mentioned, when we have no specified upper or lower constraint, then the model will default to the maximum or minimum value of the dataset. This means we need to be able to access the original dataset – which means we need the name of it, too.\nNext, we need to identify only the leaf nodes and have the path.rpart function return the sequence of split conditions that lead to each one. We do that with the following code:\n\n# finding the sequences of the leaf nodes\n  leaf &lt;- rownames(mod$frame[mod$frame$var == \"&lt;leaf&gt;\", ])\n  paths &lt;- path.rpart(mod, nodes = leaf, print.it = FALSE)\n\nThe code uses the frame of the model created using rpart in order to identify which numbered nodes are leaf nodes. Then, we store the sequences (like the one we saw earlier) of leaf node information.\n\n\n2.2.3 Writing and Running a Function to Create Bounds\nHaving done this, we can now define a helper function called get_bounds that takes a list of decision split conditions (for a single leaf node in a decision tree) and returns the rectangular boundaries – the minimum and maximum values – for the two predictor variables involved.\n\n# function to create bounds \n  get_bounds &lt;- function(leaf_paths) {\n    data &lt;- get(modelname)\n    xvals &lt;- data[[xname]]\n    \n    if (!is.null(yname)) {\n      yvals &lt;- data[[yname]]\n    } else {\n      yvals &lt;- rep(1, length(xvals))\n    }\n    \n    x_min &lt;- ifelse(min(xvals) &gt;= 0, 0, min(xvals)) ; x_max &lt;- max(xvals)\n    y_min &lt;- ifelse(min(yvals) &gt;= 0, 0, min(yvals)) ; y_max &lt;- max(yvals)\n    \n    for (i in leaf_paths) {\n      i &lt;- gsub(\" \", \"\", i)\n      if (grepl(paste0(\"^\", xname, \"&lt;\"), i)) x_max &lt;- round(min(x_max, as.numeric(sub(paste0(xname, \"&lt;\"), \"\", i))), 1)\n      if (grepl(paste0(\"^\", xname, \"&gt;=\"), i)) x_min &lt;- round(max(x_min, as.numeric(sub(paste0(xname, \"&gt;=\"), \"\", i))), 1)\n      if (!is.null(yname)) {\n        if (grepl(paste0(\"^\", yname, \"&lt;\"), i)) y_max &lt;- round(min(y_max, as.numeric(sub(paste0(yname, \"&lt;\"), \"\", i))), 1)\n        if (grepl(paste0(\"^\", yname, \"&gt;=\"), i)) y_min &lt;- round(max(y_min, as.numeric(sub(paste0(yname, \"&gt;=\"), \"\", i))), 1)\n      }\n    }\n    return(c(x_min, x_max, y_min, y_max))\n  }\n\nThat is, we begin with the individual leaf nodes and their corresponding predicted values. Then, after examining the split conditions that lead to each leaf, we determine the bounds of each predictor variable we can use to define the coordinates of the rectangles used in the graph. This contrasts with a more “top-down” approach, where one might start with the decision splits and work forward to identify the resulting prediction regions.\nThe function first pulls the original dataset used to fit the model and extracts the values of the predictor variables. It assigns these to xvals and, if a second predictor is needed, to yvals; otherwise, it fills yvals with a constant (1, for the sake of simplicity) so that the function can still compute rectangular regions in the case that only one predictor is important.\nNext, it initializes x_min, x_max, y_min, and y_max using the observed ranges of the predictor data. If all values are non-negative, it sets the minimum to 0 to create cleaner, more intuitive axis bounds; otherwise, it uses the actual minimum from the dataset.\nThe for loop then iterates through each condition in the decision path for a given node. For each of the two predictor variables, it looks for one of two possible inequality types: \\(\\geq\\) or \\(&lt;\\). It strips out spaces for each condition and checks whether each condition applies to the x or y variable, updating the relevant minimum or maximum bound accordingly. For example, a condition like \\(x \\geq 5\\) will raise x_min. If there is more than one lower or upper bound condition, it keeps the more restrictive one (because each additional condition in the path further narrows the region, and the most recent condition in the sequence overrides any earlier ones). The function finally returns the vector that we need with the rectangular region’s bounds for a given leaf node.\nWith our helper function written, we finally apply it to all of the paths of the leaf nodes. This creates a list.\n\n# apply the helper function\n  bounds_list &lt;- lapply(paths, get_bounds)\n\n\n\n2.2.4 Cleaning Up the Bounds Data Frame\nNow that we have identified the bounds of each prediction region, the most challenging part of writing this function is complete. We just need to get these bounds into a data frame that we can then ask ggplot to graph for us. It might be helpful, then, to think about what we need this data frame to look like before we begin creating it.\nTo create our alternative visualization, we would like to use the geom_rect function which, according to the ggplot2 Cheat Sheet, takes the following format:\n\ngeom_rect(aes(xmin = ..., xmax = ..., ymin = ..., ymax = ..., fill = ...)\n\nTherefore, for each individual leaf node – corresponding to one rectangular region – we would like to plot the x_min, x_max, y_min, and y_max that we created with the helper function (note the difference between xmin for the plot and x_min from the helper function). As such, we should have xmin, xmax, ymin, and ymax be individual column names in this cleaned data frame. Then, we also need an additional column in the data frame with the prediction value of each leaf node which we’ll use for both the fill color and the prediction label for each rectangular region. The following code executes on these ideas:\n\n# create a data frame with the bounds\n  bounds_df &lt;- as.data.frame(do.call(rbind, bounds_list))\n  colnames(bounds_df) &lt;- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n\n# add in the prediction values\n  leaf_preds &lt;- mod$frame[mod$frame$var == \"&lt;leaf&gt;\", \"yval\"]\n  regions &lt;- bounds_df |&gt; mutate(pred = round(leaf_preds, 1)) |&gt; tibble()\n\nThe first step is simply to turn the list that we made previously with the lapply function into a data frame. We then rename the columns. Finally, we extract predictions (stored as yval) from the leaf nodes by calling the model’s frame (which contains information about every node in the tree) and add these to the data frame.\n\n\n2.2.5 Plotting the Alternative Visualization\nWe’re very close to having our alternative visualization for decision trees! The final major step is to take the data frame that we just created and use geom_rect to create the plot. The code for the plot is shown below:\n\n# plotting the alternative visualization\n  plot &lt;- ggplot(regions) +\n    geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = pred), color = \"black\", alpha = 0.5) +\n    geom_text(aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = pred), size = 3.5) +\n    labs(x = xname, y = ifelse(is.null(yname), \"\", yname)) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\nBesides using the values in the columns from the data frame (called regions) to draw the rectangular prediction regions, the plot tries to make things visually appealing. Each rectangle is lightly color-filled based on its prediction and is labeled with that prediction in the center (finding the center of the rectangle by taking the average of the minimum and maximum values of each predictor). The x-axis and y-axis are labelled appropriately based on the predictor. The final result is our alternative visualization for decision trees!\n\n\n2.2.6 Printing the Results\nThe very last step in creating this function is to control what is returned to the user. That is, we want to give the user some flexibility: perhaps some users want alternative visualization, while others want just the data frame that contains the predictions and minimum and maximum values. The following code executes how results are printed:\n\n# control the printing of results\n  if (type == 1) {\n    return(plot)\n  } else if (type == 2) {\n    return(regions)\n  } else if (type == 3) {\n    print(plot); print(regions)\n  } else {\n    stop(\"Invalid `type` argument. Must be 1 (plot), 2 (data), or 3 (both).\")\n  }\n}\n\nWhile the default response for type is 1 (getting them just the alternative visualization plot), a user can change the type argument when using the function to get either just the data frame or both the data frame and plot if they enter 2 or 3, respectively. The if...else statements help to correctly output the user’s choice and prints an error message if a user puts in a value that doesn’t equal 1, 2, or 3.\n\n\n2.2.7 Final Alternative Visualization Code\nIf this step-by-step explanation of the alternative visualization function made it hard to picture how everything fits together – fear not! Below is the full code for the complete function, shown uninterrupted.\n\nalt_viz &lt;- function(mod, type = 1) {\n  names &lt;- unique(mod$frame$var)\n  names &lt;- names[names != \"&lt;leaf&gt;\"]\n  \n  if (length(names) == 1) {\n    xname &lt;- names[1]\n    yname &lt;- NULL\n  } else {\n    yname &lt;- names[1]\n    xname &lt;- names[2]\n  }\n  \n  modelname &lt;- mod$call$data\n  leaf &lt;- rownames(mod$frame[mod$frame$var == \"&lt;leaf&gt;\", ])\n  paths &lt;- path.rpart(mod, nodes = leaf, print.it = FALSE)\n  \n  get_bounds &lt;- function(leaf_paths) {\n    data &lt;- get(modelname)\n    xvals &lt;- data[[xname]]\n    \n    if (!is.null(yname)) {\n      yvals &lt;- data[[yname]]\n    } else {\n      yvals &lt;- rep(1, length(xvals))\n    }\n    \n    x_min &lt;- ifelse(min(xvals) &gt;= 0, 0, min(xvals)) ; x_max &lt;- max(xvals)\n    y_min &lt;- ifelse(min(yvals) &gt;= 0, 0, min(yvals)) ; y_max &lt;- max(yvals)\n    \n    for (i in leaf_paths) {\n      i &lt;- gsub(\" \", \"\", i)\n      if (grepl(paste0(\"^\", xname, \"&lt;\"), i)) x_max &lt;- round(min(x_max, as.numeric(sub(paste0(xname, \"&lt;\"), \"\", i))), 1)\n      if (grepl(paste0(\"^\", xname, \"&gt;=\"), i)) x_min &lt;- round(max(x_min, as.numeric(sub(paste0(xname, \"&gt;=\"), \"\", i))), 1)\n      if (!is.null(yname)) {\n        if (grepl(paste0(\"^\", yname, \"&lt;\"), i)) y_max &lt;- round(min(y_max, as.numeric(sub(paste0(yname, \"&lt;\"), \"\", i))), 1)\n        if (grepl(paste0(\"^\", yname, \"&gt;=\"), i)) y_min &lt;- round(max(y_min, as.numeric(sub(paste0(yname, \"&gt;=\"), \"\", i))), 1)\n      }\n    }\n    return(c(x_min, x_max, y_min, y_max))\n  }\n  \n  bounds_list &lt;- lapply(paths, get_bounds)\n  bounds_df &lt;- as.data.frame(do.call(rbind, bounds_list))\n  colnames(bounds_df) &lt;- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n  \n  leaf_preds &lt;- mod$frame[mod$frame$var == \"&lt;leaf&gt;\", \"yval\"]\n  regions &lt;- bounds_df |&gt; mutate(pred = round(leaf_preds, 1)) |&gt; tibble()\n  \n  plot &lt;- ggplot(regions) +\n    geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = pred), color = \"black\", alpha = 0.5) +\n    geom_text(aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = pred), size = 3.5) +\n    labs(x = xname, y = ifelse(is.null(yname), \"\", yname)) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n  \n  if (type == 1) {\n    return(plot)\n  } else if (type == 2) {\n    return(regions)\n  } else if (type == 3) {\n    print(plot); print(regions)\n  } else {\n    stop(\"Invalid `type` argument. Must be 1 (plot), 2 (data), or 3 (both).\")\n  }\n} # end of the function",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods and Function</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "3.1 Mock R Documentation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#mock-r-documentation",
    "href": "results.html#mock-r-documentation",
    "title": "3  Results",
    "section": "",
    "text": "3.1.1 Description\nCreates an alternative two dimensional visualization for decision trees built using the rpart package. Instead of displaying the tree structure with nodes and branches, this function maps the decision splits of at most two predictors directly onto a predictor space, dividing it into rectangular prediction regions.\n\n\n3.1.2 Usage\n\nalt_viz(mod, type = 1)\n\n\n\n3.1.3 Arguments\n\n\n\nName\nDescription\n\n\n\n\nmod\nA fitted rpart regression tree model object. The tree model should include no more than two predictors.\n\n\ntype\nAn integer (with default 1) that specifies the type of plot. Possible values:  1 returns only the plot  2 returns only the data frame with prediction region bounds and prediction values  3 returns both the plot and data frame\n\n\n\n\n\n3.1.4 Details\nThis function is designed primarily for regression models with at most two numeric predictor variables. Models should take this form for ideal use. If the model includes binary variables, they must be encoded as 0 or 1. The function is not necessarily intended for classification trees.\nThe visualization works by extracting the sequence of decision splits that lead to each leaf node, then calculating the coordinate bounds of the corresponding rectangular region in the predictor space. Each region is lightly shaded in color and labeled based on the leaf node’s predicted value.\nThe function requires that the original dataset used to fit the model is still loaded in memory, as it is referenced when extracting variable ranges.\nThe rpart and tidyverse packages must be loaded prior to using this function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#examples-of-usage",
    "href": "results.html#examples-of-usage",
    "title": "3  Results",
    "section": "3.2 Examples of Usage",
    "text": "3.2 Examples of Usage\nWe now illustrate the practical utility of the alternative visualization function by going through several examples of the function on sample data. Consistent with the requirements outlined in the mock R Documentation, let’s start by loading the two required packages, tidyverse and rpart. We will also load up the four datasets that we are using: Hitters, airquality, mtcars, and kyphosis. Since the actual meaning of these datasets isn’t directly relevant to us, we won’t spend time describing them or their variables – for more information about these datasets, please see the documentations that are hyperlinked.\n\n# load the packages\nlibrary(tidyverse)\nlibrary(rpart)\n\n# load the datasets\nHitters\ndata(airquality)\ndata(mtcars)\ndata(\"kyphosis\")\n\n\n3.2.1 Creating Plots\nWe display regression tree models for two different datasets and their corresponding plots created using the alt_viz function.\n\n3.2.1.1 Hitters (Hitters) Dataset\n\nmod1 &lt;- rpart(Logsal ~ Hits + Years, data = Hitters, cp = 0.01)\nalt_viz(mod1)\n\n\n\n\n\n\n\n\n\n\n3.2.1.2 Air Quality (airquality) Dataset\n\nmod2 &lt;- rpart(Ozone ~ Wind + Temp, data = airquality, cp = 0.01)\nalt_viz(mod2)\n\n\n\n\n\n\n\n\n\n\n3.2.1.3 Fuel Efficiency (mtcars) Dataset\n\nmod3 &lt;- rpart(mpg ~ hp + wt, data = mtcars, cp = 0.01)\nalt_viz(mod3)\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Creating Plots: Edge Cases\nThere are also a couple of edge cases – where the data or model structure diverges from the ideal scenario of continuous predictors and outcomes – that merit our discussion.\n\n3.2.2.1 One Important Predictor: Fuel Efficiency (mtcars) Dataset\nIn some cases, a regression tree might only use one of the predictors in the model. That is, one variable is important for splits. The model was designed to handle these situations.\n\nmod4 &lt;- rpart(mpg ~ hp + qsec, data = mtcars, cp = 0.01)\nalt_viz(mod4)\n\n\n\n\n\n\n\n\nHere, only the horsepower variable is important for splitting. Thus, the prediction is 25 miles per gallon if the horsepower is less than 118 and 15.8 miles per gallon if the horsepower is more than 118. The y-axis is not meaningful, except to maintain the rectangular shape.\nThis is also a good opportunity to show that the alt_viz function doesn’t require a rpart model with two predictors (remember, needing at most two predictors doesn’t mean we need exactly two predictors). For instance, we could get rid of the qsec predictor and just use the hp predictor (that is, we’ve created a model with just one predictor) and get a similar graph.\n\nmod4b &lt;- rpart(mpg ~ hp, data = mtcars, cp = 0.01)\nalt_viz(mod4b)\n\n\n\n\n\n\n\n\n\n\n3.2.2.2 Binary Predictor: Fuel Efficiency (mtcars) Dataset\nAs mentioned, binary predictors are not recommended for the dataset. However, if the binary predictor is coded with either 0 or 1, then it is possible to make the alternative visualization.\n\nmod5 &lt;- rpart(mpg ~ am + qsec, data = mtcars, cp = 0.01)\nalt_viz(mod5)\n\n\n\n\n\n\n\n\nHere, only the am variable (which takes 0 if the car is automatic and 1 if the car is manual) is important. As we can see, the alternative visualization is helpful to the extent that we know if am is less than 0.5, it’s 0 and thus classified as automatic. If it’s more than 0.5, it’s 1 and thus classified as manual. Therefore, if a car falls into the automatic prediction region (the one to the left in gray), the predicted miles per gallon is 17.1 while it’s 24.4 miles per gallon if a car falls into the manual prediction region (the one to the right in blue).\n\n\n3.2.2.3 Binary Outcome: Kyphosis (Kyphosis) Dataset\nThe alternative visualization function really isn’t designed for classification decision trees with binary outcomes and is not recommended. However, if we recode a binary outcome to have either 0 or 1, we can make the alternative visualization.\n\n# recode the outcome\ndata &lt;- kyphosis |&gt; mutate(bin = ifelse(Kyphosis == \"absent\", 0, 1))\n\n# create the alternative visualization\nmod6 &lt;- rpart(bin ~ Number + Start, data = data, cp = 0.01)\nalt_viz(mod6)\n\n\n\n\n\n\n\n\nWhile the visualization did print, it requires additional interpretation. That is, assuming a 0.5 threshold, if the prediction is below 0.5, it gets classified as 0 (in this example, that kyphosis is absent) and if the prediction is above 0.5, it gets classified as 1 (in this instance, that kyphosis is present). This adds an additional layer of complexity to the interpretation in which case it might make more sense to just use the conventional decision tree which handles classifications better.\n\n\n\n3.2.3 Changing the Output\nThe model allows for some user input with respect to what the function outputs. The default is type = 1, which produces just the plots (as we’ve seen). But there are other options, too.\n\n3.2.3.1 Data Frame Only (type = 2)\n\nalt_viz(mod = mod1, type = 2)\n\n# A tibble: 7 × 5\n   xmin  xmax  ymin  ymax  pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0   114     0     3.5   4.7\n2 114   238     0     3.5   5.3\n3   0   238     3.5   4.5   5.6\n4   0   118.    4.5   6.5   5.7\n5   0    50.5   6.5  24     5.7\n6  50.5 118.    6.5  24     6.2\n7 118.  238     4.5  24     6.7\n\n\n\n\n3.2.3.2 Data Frame and Plot Together (type = 3)\n\nalt_viz(mod = mod1, type = 3)\n\n\n\n\n\n\n\n\n# A tibble: 7 × 5\n   xmin  xmax  ymin  ymax  pred\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0   114     0     3.5   4.7\n2 114   238     0     3.5   5.3\n3   0   238     3.5   4.5   5.6\n4   0   118.    4.5   6.5   5.7\n5   0    50.5   6.5  24     5.7\n6  50.5 118.    6.5  24     6.2\n7 118.  238     4.5  24     6.7\n\n\n\n\n3.2.3.3 Errors (type = anything but 1, 2, or 3)\n\nalt_viz(mod = mod1, type = 1467)\n\nError in alt_viz(mod = mod1, type = 1467): Invalid `type` argument. Must be 1 (plot), 2 (data), or 3 (both).\n\n\n\n\n\n3.2.4 Extension: Creative Usages\n\n3.2.4.1 Plot Actual Data onto Predictor Space\nSuppose we want to compare the actual outcome value of each observed data point to the predicted region created with the rpart model. We can do this by plotting the individual, observed data points from the original dataset directly onto the predictor space created by our alternative visualization. This enables us to determine how well each observed point aligns with the predicted value (based on its region) – something which a classic decision tree can’t do.\n\n# only label 30% of the data points\nset.seed(1467)\nlabel_subset &lt;- mtcars %&gt;% slice_sample(prop = 0.3)\n\n# plot\nalt_viz(mod3) +\n  geom_point(data = mtcars, aes(x = hp, y = wt, color = mpg), size = 2) +\n  geom_text(data = label_subset, aes(x = hp, y = wt, label = round(mpg, 1)), \n            vjust = -0.7, size = 2.7641, color = \"firebrick4\")\n\n\n\n\n\n\n\n\nHere, we have the color vary for the observed data by the same color scale as the alternative visualization. That is, if the prediction and observed value are close, they will share the same intensity of blue – that is, a lighter blue dot will correspond to a lighter blue region (and vice-versa). Additionally, we label some of the data points so we compare their exact observed value to their prediction region value. These help us to visually assess how well the regression tree using rpart captures the structure of the data.\nIn this case, we see that most observed mpg values fall relatively close to the predicted region averages, suggesting that mod3 generally partitions the predictor space effectively. For example, if we look at the bottom light blue region, most of the data points that fall within that region tend to also be a lighter blue. Furthermore, the labeled values (27.3 and 30.4 miles per gallon) are also relatively close to the predicted value of 29 miles per gallon.\n\n\n3.2.4.2 Changing the Plot Color\nSuppose a user really doesn’t like the color blue and doesn’t want the alternative visualization to be shaded in with blue. I apologize to that person, but blue is my favorite color – and so, if they want to get rid of the blue, they have to remake the chart from scratch. Luckily, this isn’t that difficult if we just extract the data frame used to make the graph with the type = 2 argument in the alt_viz function. Once that’s done, we just need to replot with the similar code that modifies the color (let’s say to black and white).\n\n# extract the regions\nregions &lt;- alt_viz(mod1, type = 2)\n\n# replot the regions with the new color\nggplot(regions) +\n    geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = pred), color = \"black\", alpha = 0.5) +\n    geom_text(aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = pred), size = 3.5) +\n    labs(x = \"Hits\", y = \"Years\") +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "improvements.html",
    "href": "improvements.html",
    "title": "4  Future Directions",
    "section": "",
    "text": "4.1 Improve Code Efficiency\nWhile the code underlying the alt_viz function runs well, it could benefit from increased efficiency. That is, there are a couple of areas where we could “clean-up” the code, making it more concise and – more importantly – more efficient.\nPerhaps the best example of where the code could be improved is in eventually eliminating the use of nested helper functions. As we might recall from our discussion in the “Methods” section, the alt_viz function relies on a new helper function that we created called get_bounds to extract rectangular region boundaries from the sequences produced by path.rpart. However, to improve both the clarity and efficiency of the code, the function would benefit from further modularization: organizing the code into separate, top-level functions rather than nesting them. This approach is widely considered to be coding best practice because it improves efficiency, enhances the readability of the function’s code, simplifies testing and debugging, and makes the codebase easier to maintain and build upon in the future. Therefore, future iterations of this project would want to try to “unnest” the get_bounds function from the alt_viz function and keep them separate instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Future Directions</span>"
    ]
  },
  {
    "objectID": "improvements.html#handle-edge-cases-more-robustly",
    "href": "improvements.html#handle-edge-cases-more-robustly",
    "title": "4  Future Directions",
    "section": "4.2 Handle Edge Cases More Robustly",
    "text": "4.2 Handle Edge Cases More Robustly\nWe discussed some of these “edge cases” already, but there are certainly a couple of more that immediately come to mind (and perhaps others that haven’t come to mind yet, either!). Most obviously, the alt_viz function doesn’t really know what to do when we have a model with more than two predictors.\nSometimes, the model is able to produce an accurate alternative visualization if only two of the predictors are important. Indeed, the function can still produce an accurate and interpretable visualization if the model relies primarily on two predictors (that is, only two predictors are important for splits). In these cases, the output remains interpretable because the splits based on two predictors is consistent with the two dimensional predictor space the alternative visualization is intended to represent. This situation is not ideal – because, as a best practice, we really shouldn’t be making the alternative visualization with models that have more than two predictors in the first place.\nThe real problem, however, is if a user were to create a model with more than two predictors and more than two predictors were important for data splits. At that point, the alt_viz function has no way to represent the higher dimensional structure (imagine trying to draw a four dimensional space – impossible). Consequently, any resulting visualization that the alt_viz function produces under these circumstances would be misleading or incomplete. For example, in the following model, the Wind, Temp, and Solar.R variables are all important for creating splits in order to predict the Ozone level. The resulting alternative visualization function is weird.\n\nmod_bad &lt;- rpart(Ozone ~ Wind + Temp + Solar.R, data = airquality, cp = 0.01)\nalt_viz(mod_bad)\n\n\n\n\n\n\n\n\nAs we can see, the fact that there are three important predictors for decision splits has made this alternative visualization clearly inaccurate. Most notably, in the bottom right most prediction region there are actually two predictions – how are we supposed to interpret this? We can’t.\nIn this way, we can see how it would be helpful if the alt_viz function could help enforce the “at most two predictors” condition for the regression tree model. For future versions of the alt_viz function, it would be helpful if it could throw an error message in the event that a user tried to use the alt_viz function on an rpart model that had more than three predictors.\nFinally – and we alluded to this earlier – the other “edge case” that the alt_viz could handle better is binary variables. It would be nice for future versions of the alt_viz function to be able to better handle classification models or binary predictors that are not coded as either 0 or 1. Ideally, the alt_viz function would be able to recognize these types of variables and either convert them internally to 0 and 1, or the alt_viz function would be able to adapt the plotting logic accordingly. But, if that isn’t feasible, it would be helpful if the alt_viz model could output a clear and informative error message about the situation. Currently, attempting to run the alt_viz function on a regression tree model with binary predictors not coded as either 0 or 1 produces the following unhelpful message:\n\nmod_bad2 &lt;- rpart(Logsal ~ Hits + Division, data = Hitters, cp = 0.01)\nalt_viz(mod_bad2)\n\nError in Summary.factor(structure(c(2L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, : 'min' not meaningful for factors",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Future Directions</span>"
    ]
  },
  {
    "objectID": "improvements.html#allow-for-greater-user-customization",
    "href": "improvements.html#allow-for-greater-user-customization",
    "title": "4  Future Directions",
    "section": "4.3 Allow for Greater User Customization",
    "text": "4.3 Allow for Greater User Customization\nCurrently, the user’s ability to customize the output of the alt_viz function is limited to the type of output (plot, data frame, or both). However, as we saw in the last example of the previous “Results” section, making any visual edits to the default alternative visualization – such as changing the color, tick marks, or transparency, or adding titles, subtitles, captions, and other annotations – is not currently possible. To make these desired changes, we have to call the raw data frame and then code the customized graph ourselves. Therefore, a future improvement would increase the user’s ability to customize the alternative visualization within the alt viz function, without having to manually do it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Future Directions</span>"
    ]
  },
  {
    "objectID": "improvements.html#publish-the-function-in-a-r-package",
    "href": "improvements.html#publish-the-function-in-a-r-package",
    "title": "4  Future Directions",
    "section": "4.4 Publish the Function in a R Package",
    "text": "4.4 Publish the Function in a R Package\nFinally, the alt_viz function has utility beyond this project! The function could be helpful to statisticians, educators, or data analysts looking to visually interpret decision tree plots in a more intuitive way. The best and most straightforward way to enable these people to use the alt_viz function is to publish a R package that contains the alt_viz function. This way, the function would be readily accessible to those interested in using the function – and those who could even contribute to its continued development!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Future Directions</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "To conclude this project, let’s return to the initial question motivating our research and provide a definitive answer to it:\n\nTo what extent can we use R to design a function that creates an alternative visualization for decision trees with one or two numerical predictors – specifically, one that takes the statistical information typically used to build a conventional decision tree and instead maps the decision splits directly onto a predictor space?\n\nTo put it simply, the answer is: yes, we can use R to build the alternative visualization. The alt_viz function produced is a working proof of concept that R is not just capable of this task, but that it does it pretty well! By using the path.rpart function, we can extract and leverage the statistical information typically used to build classic regression decision trees with the rpartfunction. More specifically, we can use the sequence of decision rules for each leaf node, reorganize those rules into rectangular coordinate bounds, and generate a clean visual representation of each prediction region. The end result is a function that automatically maps the decision splits of a regression tree model onto a predictor space (or provides a data frame containing the coordinates of the rectangular regions that comprise the predictor space).\nIn addition to demonstrating that such a function can be built using R, this project underscores the strengths of the alternative visualization that originally motivated the development of the alt_viz function. Unlike conventional decision tree diagrams, which – especially when they are deep and wide – can be abstract and confusing to trace, the alternative visualization is much more concrete. It shows us a two dimensional predictor space with prediction regions composed of rectangles whose bounds are determined by specific values of the predictors. This makes it straightforward to take any combination of two predictor values and immediately see the model’s prediction. Because of this, the alternative visualization (and, thus, the alt_viz function) is particularly helpful for statisticians or data analysts are interested in communicating simple regression tree results to audiences less familiar with statistics or, as we did in class, for educators trying to teach core ideas behind decision tree models. Furthermore, as we explored in this project, the alternative visualization allows us to juxtapose observed data and predictions in ways that we can’t with decision trees. For instance, we can plot observed data points from the original dataset directly onto the predictor space created by our alternative visualization to see how well the predictions match with the actual data. While the alternative visualization does have its limitations (including being restricted to models with at most two numeric predictors), these broader applications – both in communication and statistical analysis – highlight the utility of the alt_viz function.\nFinally, on a personal note, I learned a lot from creating the alt_viz function (and also had fun just practicing my R coding skills). This project was, in many ways, an exercise in taking a complex problem (turning a visualization into code), breaking it down into its component parts, asking myself what information I needed, getting that information, and then using that information to build the final product. In particular, I’d say the most challenging part was figuring out how I would convert the statistical information in an rpart model that’s typically used to build the classic decision tree into a data frame that I could then use to build the alternative visualization. It was a tedious process and one that involved several days of following various (unsuccessful) leads. But eventually, after enough research online and with some ChatGPT, I discovered that the path.rpart function was my best entry point. After I figured this out, building the rest of the function was more straightforward.\nI don’t intend for this to be the end of this project. At some point, perhaps when I get tired of reading cases and writing briefs at law school next year, I hope to return to this project and try to work on some of the “future directions” that I laid out in the previous section. I think it would be a great way to keep up my R coding skills. I’m excited for it!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]