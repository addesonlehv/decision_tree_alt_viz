[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Tree Alternative Visualization",
    "section": "",
    "text": "1 Introduction\nConventional decision tree visualizations – complete with nodes, branches, and leaves – are an invaluable tool in statistical modeling. They help us understand regression “splits” (that is, how data is partitioned based on independent variables) and make predictions about new, unseen data. In other words, decision trees are an effective way to take a data point and follow a sequence of decision rules (thus creating the “tree” shape) to ultimately arrive at a predicted value for the outcome variable. But decision tree plots aren’t the only way that we could, or should, visualize how a model makes regression decisions.\nIndeed, the classic decision tree has some interpretability challenges inherent in the node and branch structure. Particularly when the decision trees are wide and deep, these visualizations can be tricky to follow and (ultimately) make predictions off of. When we have two (non-binary) predictor variables, we can create an alternative visualization that basically maps the individual decision splits directly onto a predictor space with two variables – one predictor on the x-axis and another on the y-axis – so that we can see how and where the decision tree partitions the data:\n\n\n\nAlternative visualization of a decision tree by Professor Robbins\n\n\nAs we can see, the space is divided into rectangular regions according to the splits made by the tree – with each region corresponding to a terminal node (or leaf) of a decision tree and is labeled with the predicted value assigned to that region. The vertical and horizontal lines show the values at which the model splits the data. Ultimately, the advantage of this visualization method is that it provides an immediate and (in my opinion, at least) more intuitive understanding of how the regression model actually partitions the predictor space, making it more straightforward to apply the model to make predictions about new data, without necessarily having to trace through a complex tree. This alternative visualization method also allows us to make other (cool) informative graphs that wouldn’t be possible with the conventional decision tree: for example, we could plot individual data points from the original dataset directly onto the predictor space created by our alternative visualization, making it easy to compare each point to the predicted value assigned to its region.\nHowever, the reason that this sample visualization was hand drawn (rather than done digitally in R) is because there isn’t really a simple, readily available way to make these visualizations automatically in R. This project is a pilot study attempt to address that issue. To that end, the project will be motivated by the following question: how can we use R to design a function that creates an alternative visualization for decision trees with one or two (non-binary) predictors – specifically, one that takes the statistical information typically used to build a conventional decision tree and instead maps the decision splits directly onto a predictor space (as shown in our example)?\nThe next four sections of this website dive deeper into this question. The subsequent section describes the method and approach for creating this function, walking through step-by-step how the function in R works. Then, we’ll take a look at how to use the function and illustrate its usage using sample data from a variety of datasets. Finally, the final two sections will outline future directions for this pilot project and provide some concluding thoughts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods",
    "section": "",
    "text": "2.1 Coding Approach\nPresumably, there are many possible ways to go about creating this alternative visualization function. So where should we start? When looking at the decision splits in the sample alternative visualization, there seemed to really be two different ways we could go about recreating this graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-approach",
    "href": "methods.html#coding-approach",
    "title": "2  Methods",
    "section": "",
    "text": "2.1.1 Approach 1\nFirst, we could think about the splits as being created by horizontal and vertical lines with each line representing a decision split. If we were careful to truncate each line so that it only spans the region where the corresponding split condition applies, we can use these boundaries to define predictions regions, which we could then label with the prediction value. This method seemed somewhat challenging to implement in R, however – especially considering that we’d need to be able to effectively truncate each line.\n\n\n2.1.2 Approach 2\nThe second approach, then, is to think about each of the prediction regions as rectangles. Each of these rectangles would have coordinates defined by the minimum and maximum values of the predictor variables that satisfy the split conditions leading to a particular leaf. For example, if we look at the sample alternative visualization with the prediction 5.3, we can see that the minimum x-value that would satisfy the prediction is 4 years and the maximum value is 5 years. Similarly, the minimum y-value that would satisfy the prediction is 0 hits and the maximum value is 125 hits. This, in turn, gives us coordinates of a rectangle – (4, 0), (4, 125), (5, 0), and (5, 125) – that represents the prediction region. In other words, by tracing the sequence of decision rules for each leaf node, we can derive the exact bounds of the region it occupies in the predictor space – and subsequently use those bounds to draw the rectangle and assign its predicted value.\n That is, we begin with the individual leaf nodes and their corresponding predicted values. Then, after examining the split conditions that lead to each leaf, we determine the bounds of each predictor variable we can use to define the coordinates of the rectangles used in the graph. This contrasts with a more “top-down” approach, where one might start with the decision splits and work forward to identify the resulting prediction regions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-the-function",
    "href": "methods.html#coding-the-function",
    "title": "2  Methods",
    "section": "2.2 Coding the Function",
    "text": "2.2 Coding the Function\nWith this general approach in mind, we ultimately know what goal we are aiming for: some sort of geom_rect (rectangle) function that makes a bunch of rectangles based on the coordinates set by the decision split bounds. Thus, we first need to somehow extract each prediction region’s decision split bounds and each region’s corresponding prediction. Before we begin though, we need to do a little bit of “housekeeping” to set up the function and get the information that we need.\n\n2.2.1 Housekeeping\nThe first step is to simply set up the function. We will call our function alt_viz (short for alternative visualization) and it will take two inputs. The first input, mod, is a regression model that is created using the the rpart function in the rpart package – thus making the rpart package a requirement for using the function. The second input, type, is a numeric value that corresponds to 1, 2, or 3. We’ll talk about this more later, but it allows the user to control what type of output they receive from running the function. This is the first line of code of our function:\n\n# calling the function\nalt_viz &lt;- function(mod, type = 1) {\n\nNow, we need to figure out how to extract the necessary information from the model that is created using the rpart function. We know that the model created contains the information needed to build the classic decision tree – since the rpart.plot function, which makes these conventional visualizations, also uses this model. This is where we need to be a little creative. A little bit of research tells us that there’s a function called path.rpart, which retrieves a sequence of split conditions – that is, the decision rules – that lead from the root of the tree to each specified node. For example, using the same baseball data that was used in the sample alternative visualization, the result of the path.rpart function for one (leaf) node looks like this:\n\nnode number:27 root  Years&gt;=4.5  Hits&lt; 117.5 Years&gt;=6.5 Hits&gt;=50.5\n\nThis sequence should tell us everything that we need to know about the split decisions for this particular (leaf) node: after the root, if the number of years a player has been playing baseball is greater than or equal to 4.5 years, and they have fewer than 117.5 hits, and have been playing for at least 6.5 years, and have 50.5 or more hits, then they fall into this specific prediction region. This mean that we can create the bounds of the rectangular prediction region for this node – and other nodes – if we do the following:\n\nTurn the sequencing rules into a vector that contains the minimum and maximum values for the Hits and Years predictors. For example, we know that the minimum value for Hits in this prediction region is 50.5 and the maximum value is 117.5. Some additional points to consider:\n\nIf there is more than one lower or upper bound conditions, like \\(Years \\geq 4.5\\) and \\(Years \\geq 6.5\\), then we need to keep the more restrictive condition – \\(Years \\geq 6.5\\), in this instance – since it defines a narrower region.\nIf there is no explicitly specified inequality, like \\(Years &lt; {}...{}\\), then we should assume that there is no upper or lower constraint in that direction and, thus, should fall back on the reasonable default of the maximum or minimum value of the dataset.\nIf we are interested in generalizability to new datasets (which we are), then we need to design the code logic such that it doesn’t rely on any specific dataset predictor names. Rather, it should flexibly adapt to new trees and new data.\n\nIdentify only the leaf nodes and have the path.rpart function return the sequence of split conditions that lead to each one.\nApply a function that applies what the first bullet point suggests to all of our leaf nodes. This will create a list for each leaf node and its relevant rectangle boundaries for their prediction regions.\n\nWe can take these principles and turn them into code. But first, just a little bit more housekeeping.\n\n\n2.2.2 More Housekeeping\nAs we saw, we need to design our code so that it’s not reliant on one specific dataset. This means writing it in a way that dynamically adapts to different variable names, ranges, and structures without hard-coded assumptions. The next couple of lines of the function executes on this:\n\n# extracting model specific information\n  names &lt;- unique(mod$frame$var)\n  names &lt;- names[names != \"&lt;leaf&gt;\"]\n\n# account for the case where only one predictor is important\n  if (length(names) == 1) {\n    xname &lt;- names[1]\n    yname &lt;- NULL\n  } else {\n    yname &lt;- names[1]\n    xname &lt;- names[2]\n  }\n\n# find the name of the original dataset\n  modelname &lt;- mod$call$data\n\nThe function first extracts the list of names of the predictors (like Years and Hits) by looking at what variable names are produced. We also need to remove the &lt;leaf&gt; name from this list as it’s not a predictor.\nThen, we want to account for the fact that even if we include two predictors in our regression model, perhaps only one of them is actually important for splits. The if...else statement assigns the vertical predictor value (yname) to NULL. We’ll deal with this part more later.\nFinally, we need to extract the name of the original dataset that the model used. As we mentioned, when we have no specified upper or lower constraint, then model will default to the maximum or minimum value of the dataset. This means we need to be able to access the original dataset – which means we need the name of it, too.\nNext, we need to identify only the leaf nodes and have the path.rpart function return the sequence of split conditions that lead to each one. We do that with the following code:\n\n# finding the sequences of the leaf nodes\n  leaf &lt;- rownames(mod$frame[mod$frame$var == \"&lt;leaf&gt;\", ])\n  paths &lt;- path.rpart(mod, nodes = leaf, print.it = FALSE)\n\nThe code uses the frame of the model created using rpart in order to identify which numbered nodes are leaf nodes. Then, we store the sequences (like the one we saw earlier) of leaf node information.\n\n\n2.2.3 Writing and Running a Function to Create Bounds\nHaving done this, we can now define a helper function called get_bounds that takes a list of decision split conditions (for a single terminal node in a decision tree) and returns the rectangular boundaries – the minimum and maximum values – for the two predictor variables involved.\n\n# function to create bounds \n  get_bounds &lt;- function(leaf_paths) {\n    data &lt;- get(modelname)\n    xvals &lt;- data[[xname]]\n    \n    if (!is.null(yname)) {\n      yvals &lt;- data[[yname]]\n    } else {\n      yvals &lt;- rep(1, length(xvals))\n    }\n    \n    x_min &lt;- ifelse(min(xvals) &gt;= 0, 0, min(xvals)) ; x_max &lt;- max(xvals)\n    y_min &lt;- ifelse(min(yvals) &gt;= 0, 0, min(yvals)) ; y_max &lt;- max(yvals)\n    \n    for (i in leaf_paths) {\n      i &lt;- gsub(\" \", \"\", i)\n      if (grepl(paste0(\"^\", xname, \"&lt;\"), i)) x_max &lt;- round(min(x_max, as.numeric(sub(paste0(xname, \"&lt;\"), \"\", i))), 1)\n      if (grepl(paste0(\"^\", xname, \"&gt;=\"), i)) x_min &lt;- round(max(x_min, as.numeric(sub(paste0(xname, \"&gt;=\"), \"\", i))), 1)\n      if (!is.null(yname)) {\n        if (grepl(paste0(\"^\", yname, \"&lt;\"), i)) y_max &lt;- round(min(y_max, as.numeric(sub(paste0(yname, \"&lt;\"), \"\", i))), 1)\n        if (grepl(paste0(\"^\", yname, \"&gt;=\"), i)) y_min &lt;- round(max(y_min, as.numeric(sub(paste0(yname, \"&gt;=\"), \"\", i))), 1)\n      }\n    }\n    return(c(x_min, x_max, y_min, y_max))\n  }\n\nThat is, we begin with the individual terminal nodes and their corresponding predicted values. Then, after examining the split conditions that lead to each leaf, we determine the bounds of each predictor variable we can use to define the coordinates of the rectangles used in the graph. This contrasts with a more “top-down” approach, where one might start with the decision splits and work forward to identify the resulting prediction regions.\nThe function first pulls the original dataset used to fit the model and extracts the values of the predictor variables. It assigns these to xvals and, if a second predictor is needed, to yvals; otherwise, it fills yvals with a constant (1, for the sake of simplicity) so that the function can still compute rectangular regions in the case that only one predictor is important.\nNext, it initializes x_min, x_max, y_min, and y_max using the observed ranges of the predictor data. If all values are non-negative, it sets the minimum to 0 to create cleaner, more intuitive axis bounds; otherwise, it uses the actual minimum from the dataset.\nThe for loop then iterates through each condition in the decision path for a given node. For each of the two predictor variables, it looks for one of two possible inequality types: \\(\\geq\\) or \\(&lt;\\). It strips out spaces for each condition and checks whether each condition applies to the x or y variable, updating the relevant minimum or maximum bound accordingly. For example, a condition like \\(x \\geq 5\\) will raise x_min. If there is more than one lower or upper bound condition, it keeps the more restrictive one (because each additional condition in the path further narrows the region, and the most recent condition in the sequence overrides any earlier ones). The function finally returns the vector that we need with the rectangular region’s bounds for a given leaf node.\nWith our helper function written, we finally apply it to all of the paths of the leaf nodes. This creates a list.\n\n# apply the helper function\n  bounds_list &lt;- lapply(paths, get_bounds)\n\n\n\n2.2.4 Cleaning Up the Bounds Data Frame\nNow that we have identified the bounds of each prediction region, the most challenging part of writing this function is complete. We just need to get these bounds into a data frame that we can then ask ggplot to graph for us. It might be helpful, then, to think about what we need this data frame to look like before we begin creating it.\nTo create our alternative visualization, we would like to use the geom_rect function which, according to the ggplot2 Cheat Sheet, takes the following format:\n\ngeom_rect(aes(xmin = ..., xmax = ..., ymin = ..., ymax = ..., fill = ...)\n\nTherefore, for each individual leaf node – corresponding to one rectangular region – we would like to plot the x_min, x_max, y_min, and y_max that we created with the helper function (note the difference between xmin for the plot and x_min from the helper function). As such, we should have xmin, xmax, ymin, and ymax be individual column names in this cleaned data frame. Then, we also need an additional column in the data frame with the prediction value of each leaf node which we’ll use for both the fill color and the prediction label for each rectangular region. The following code executes on these ideas:\n\n# create a data frame with the bounds\n  bounds_df &lt;- as.data.frame(do.call(rbind, bounds_list))\n  colnames(bounds_df) &lt;- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n\n# add in the prediction values\n  leaf_preds &lt;- mod$frame[mod$frame$var == \"&lt;leaf&gt;\", \"yval\"]\n  regions &lt;- bounds_df |&gt; mutate(pred = round(leaf_preds, 1)) |&gt; tibble()\n\nThe first step is simply to turn the list that we made previously with the lapply function into a data frame. We then rename the columns. Finally, we extract predictions (stored as yval) from the leaf nodes by calling the model’s frame (which contains information about every node in the tree) and add these to the data frame.\n\n\n2.2.5 Plotting the Alternative Visualization\nWe’re very close to having our alternative visualization for decision trees! The final major step is to take the data frame that we just created and using geom_rect to create the plot. The code for the plot is shown below:\n\n# plotting the alternative visualization\n  plot &lt;- ggplot(regions) +\n    geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = pred), color = \"black\", alpha = 0.5) +\n    geom_text(aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = pred), size = 3.5) +\n    labs(x = xname, y = ifelse(is.null(yname), \"\", yname)) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\nBesides using the values in the columns from the data frame (called regions) to draw the rectangular prediction regions, the plot tries to make things visually appealing. Each rectangle is lightly color-filled based on its prediction and is labeled with that prediction in the center (finding the center of the rectangle by taking the average of the minimum and maximum values of each predictor). The x-axis and y-axis are labelled appropriately based on the predictor. The final result is our alternative visualization for decision trees!\n\n\n2.2.6 Printing the Results\nThe very last step in creating this function is to control what returned to the user. That is, we want to give the user some flexibility: perhaps some users want alternative visualization, while others want just the data frame that contains the predictions and minimum and maximum values. The following code executes how results are printed:\n\n# control the printing of results\n  if (type == 1) {\n    print(plot)\n  } else if (type == 2) {\n    print(regions)\n  } else if (type == 3) {\n    print(plot); print(regions)\n  } else {\n    stop(\"Invalid `type` argument. Must be 1 (plot), 2 (data), or 3 (both).\")\n  }\n}\n\nWhile the default response for type is 1 (getting them just the alternative visualization plot), a user can change the type argument when using the function to get either just the data frame or both the data frame and plot if they enter 2 or 3, respectively. The if...else statements help to correctly output the user’s choice and prints an error message if a user puts in a value that doesn’t equal 1, 2, or 3.\n\n\n2.2.7 Final Alternative Visualization Code\nIf this step-by-step explanation of the alternative visualization function made it hard to picture how everything fits together – fear not! Below is the full code for the complete function, shown uninterrupted.\n\nalt_viz &lt;- function(mod, type = 1) {\n  names &lt;- unique(mod$frame$var)\n  names &lt;- names[names != \"&lt;leaf&gt;\"]\n  \n  if (length(names) == 1) {\n    xname &lt;- names[1]\n    yname &lt;- NULL\n  } else {\n    yname &lt;- names[1]\n    xname &lt;- names[2]\n  }\n  \n  modelname &lt;- mod$call$data\n  leaf &lt;- rownames(mod$frame[mod$frame$var == \"&lt;leaf&gt;\", ])\n  paths &lt;- path.rpart(mod, nodes = leaf, print.it = FALSE)\n  \n  get_bounds &lt;- function(leaf_paths) {\n    data &lt;- get(modelname)\n    xvals &lt;- data[[xname]]\n    \n    if (!is.null(yname)) {\n      yvals &lt;- data[[yname]]\n    } else {\n      yvals &lt;- rep(1, length(xvals))\n    }\n    \n    x_min &lt;- ifelse(min(xvals) &gt;= 0, 0, min(xvals)) ; x_max &lt;- max(xvals)\n    y_min &lt;- ifelse(min(yvals) &gt;= 0, 0, min(yvals)) ; y_max &lt;- max(yvals)\n    \n    for (i in leaf_paths) {\n      i &lt;- gsub(\" \", \"\", i)\n      if (grepl(paste0(\"^\", xname, \"&lt;\"), i)) x_max &lt;- round(min(x_max, as.numeric(sub(paste0(xname, \"&lt;\"), \"\", i))), 1)\n      if (grepl(paste0(\"^\", xname, \"&gt;=\"), i)) x_min &lt;- round(max(x_min, as.numeric(sub(paste0(xname, \"&gt;=\"), \"\", i))), 1)\n      if (!is.null(yname)) {\n        if (grepl(paste0(\"^\", yname, \"&lt;\"), i)) y_max &lt;- round(min(y_max, as.numeric(sub(paste0(yname, \"&lt;\"), \"\", i))), 1)\n        if (grepl(paste0(\"^\", yname, \"&gt;=\"), i)) y_min &lt;- round(max(y_min, as.numeric(sub(paste0(yname, \"&gt;=\"), \"\", i))), 1)\n      }\n    }\n    return(c(x_min, x_max, y_min, y_max))\n  }\n  \n  bounds_list &lt;- lapply(paths, get_bounds)\n  bounds_df &lt;- as.data.frame(do.call(rbind, bounds_list))\n  colnames(bounds_df) &lt;- c(\"xmin\", \"xmax\", \"ymin\", \"ymax\")\n  \n  leaf_preds &lt;- mod$frame[mod$frame$var == \"&lt;leaf&gt;\", \"yval\"]\n  regions &lt;- bounds_df |&gt; mutate(pred = round(leaf_preds, 1)) |&gt; tibble()\n  \n  plot &lt;- ggplot(regions) +\n    geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = pred), color = \"black\", alpha = 0.5) +\n    geom_text(aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = pred), size = 3.5) +\n    labs(x = xname, y = ifelse(is.null(yname), \"\", yname)) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n  \n  if (type == 1) {\n    print(plot)\n  } else if (type == 2) {\n    print(regions)\n  } else if (type == 3) {\n    print(plot); print(regions)\n  } else {\n    stop(\"Invalid `type` argument. Must be 1 (plot), 2 (data), or 3 (both).\")\n  }\n} # end of the function",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "improvements.html",
    "href": "improvements.html",
    "title": "4  Future Updates",
    "section": "",
    "text": "make coding better/more efficient – no nested statements\nfixing errors for the edge cases\nputting this into a package\nallowing for more user customization",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Future Updates</span>"
    ]
  }
]