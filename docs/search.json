[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Tree Alternative Visualization",
    "section": "",
    "text": "1 Introduction\nConventional decision tree visualizations – complete with nodes, branches, and leaves – are an invaluable tool in statistical modeling. They help us understand regression “splits” (that is, how data is partitioned based on independent variables) and make predictions about new, unseen data. In other words, decision trees are an effective way to take a data point and follow a sequence of decision rules (thus creating the “tree” shape) to ultimately arrive at a predicted value for the outcome variable. But decision tree plots aren’t the only way that we could, or should, visualize how a model makes regression decisions.\nIndeed, the classic decision tree has some interpretability challenges inherent in the node and branch structure. Particularly when the decision trees are wide and deep, these visualizations can be tricky to follow and (ultimately) make predictions off of. When we have two (non-binary) predictor variables, we can create an alternative visualization that basically maps the individual decision splits directly onto a predictor space with two variables – one predictor on the x-axis and another on the y-axis – so that we can see how and where the decision tree partitions the data:\n\n\n\nAlternative visualization of a decision tree by Professor Robbins\n\n\nAs we can see, the space is divided into rectangular regions according to the splits made by the tree – with each region corresponding to a terminal node (or leaf) of a decision tree and is labeled with the predicted value assigned to that region. The vertical and horizontal lines show the values at which the model splits the data. Ultimately, the advantage of this visualization method is that it provides an immediate and (in my opinion, at least) more intuitive understanding of how the regression model actually partitions the predictor space, making it more straightforward to apply the model to make predictions about new data, without necessarily having to trace through a complex tree. This alternative visualization method also allows us to make other (cool) informative graphs that wouldn’t be possible with the conventional decision tree: for example, we could plot individual data points from the original dataset directly onto the predictor space created by our alternative visualization, making it easy to compare each point to the predicted value assigned to its region.\nHowever, the reason that this sample visualization was hand drawn (rather than done digitally in R) is because there isn’t really a simple, readily available way to make these visualizations automatically in R. This project is a pilot study attempt to address that issue. To that end, the project will be motivated by the following question: how can we use R to design a function that creates an alternative visualization for decision trees with one or two (non-binary) predictors – specifically, one that takes the statistical information typically used to build a conventional decision tree and instead maps the decision splits directly onto a predictor space (as shown in our example)?\nThe next four sections of this website dive deeper into this question. The subsequent section describes the method and approach for creating this function, walking through step-by-step how the function in R works. Then, we’ll take a look at how to use the function and illustrate its usage using sample data from a variety of datasets. Finally, the final two sections will outline future directions for this pilot project and provide some concluding thoughts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods",
    "section": "",
    "text": "2.1 Coding Approach\nPresumably, there are many possible ways to go about creating this alternative visualization function. So where should we start? When looking at the decision splits in the sample alternative visualization, there seemed to really be two different ways we could go about recreating this graph.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-approach",
    "href": "methods.html#coding-approach",
    "title": "2  Methods",
    "section": "",
    "text": "2.1.1 Approach 1\nFirst, we could think about the splits as being created by horizontal and vertical lines with each line representing a decision split. If we were careful to truncate each line so that it only spans the region where the corresponding split condition applies, we can use these boundaries to define predictions regions, which we could then label with the prediction value. This method seemed somewhat challenging to implement in R, however – especially considering that we’d need to be able to effectively truncate each line.\n\n\n2.1.2 Approach 2\nThe second approach, then, was to think about each of the prediction regions as rectangles. Each of these rectangles would have coordinates defined by the minimum and maximum values of the predictor variables that satisfy the split conditions leading to a particular leaf. For example, if we look at the sample alternative visualization with the prediction 5.3, we can see that the minimum x-value that would satisfy the prediction is 4 years and the maximum value is 5 years. Similarly, the minimum y-value that would satisfy the prediction is 0 hits and the maximum value is 125 hits. This, in turn, gives us coordinates of a rectangle – (4, 0), (4, 125), (5, 0), and (5, 125) – that represents the prediction region. In other words, by tracing the sequence of decision rules for each leaf node, we can derive the exact bounds of the region it occupies in the predictor space – and subsequently use those bounds to draw the rectangle and assign its predicted value.\n That is, we begin with the individual leaf nodes and their corresponding predicted values. Then, after examining the split conditions that lead to each leaf, we determine the bounds of each predictor variable we can use to define the coordinates of the rectangles used in the graph. This contrasts with a more “top-down” approach, where one might start with the decision splits and work forward to identify the resulting prediction regions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#coding-the-function",
    "href": "methods.html#coding-the-function",
    "title": "2  Methods",
    "section": "2.2 Coding the Function",
    "text": "2.2 Coding the Function\nWith this general approach in mind, we ultimately know what goal we are aiming for: some sort of geom_rect (rectangle) function that makes a bunch of rectangles based on the coordinates set by the decision split bounds. Thus, we first need to somehow extract each prediction region’s decision split bounds and each region’s corresponding prediction. Before we begin though, we need to do a little bit of “housekeeping” to set up the function and get the information that we need.\n\n2.2.1 Housekeeping\nThe first step is to simply set up the function. We will call our function alt_viz (short for alternative visualization) and it will take two inputs. The first input, mod, is a regression model that is created using the the rpart function in the rpart package – thus making the rpart package a requirement for using the function. The second input, type, is a numeric value that corresponds to 1, 2, or 3. We’ll talk about this more later, but it allows the user to control what type of output they receive from running the function. This is the first line of code of our function:\n\n## chunk 1: calling the function\n\nNow, we need to figure out how to extract the necessary information from the model that is created using the rpart function. We know that the model created contains the information needed to build the classic decision tree – since the rpart.plot function, which makes these conventional visualizations, also uses this model. This is where we need to be a little creative. A little bit of research tells us that there’s a function called path.rpart, which retrieves a sequence of split conditions – that is, the decision rules – that lead from the root of the tree to each specified node. For example, using the same baseball data that was used in the sample alternative visualization, the result of the path.rpart function for one (leaf) node looks like this:\nnode number:27 root  Years&gt;=4.5  Hits&lt; 117.5 Years&gt;=6.5 Hits&gt;=50.5\nThis sequence should tell us everything that we need to know about the split decisions for this particular (leaf) node: after the root, if the number of years a player has been playing baseball is greater than or equal to 4.5 years, and they have fewer than 117.5 hits, and have been playing for at least 6.5 years, and have 50.5 or more hits, then they fall into this specific prediction region. This mean that we can create the bounds of the rectangular prediction region for this node – and other nodes – if we do the following:\n\nTurn the sequencing rules into a vector that contains the minimum and maximum values for the Hits and Years predictors. For example, we know that the minimum value for Hits in this prediction region is 50.5 and the maximum value is 117.5. Some additional points to consider:\n\nIf there is more than one lower or upper bound conditions, like \\(Years \\geq 4.5\\) and \\(Years \\geq 6.5\\), then we need to keep the more restrictive condition – \\(Years \\geq 6.5\\), in this instance – since it defines a narrower region.\nIf there is no explicitly specified inequality, like \\(Years &lt; {}...{}\\), then we should assume that there is no upper or lower constraint in that direction and, thus, should fall back on the reasonable default of the maximum or minimum value of the dataset for the purposes.\nIf we are interested in generalizability to new datasets (which we are), then we need to design the code logic such that it doesn’t rely on any specific dataset predictor names. Rather, it should flexibly adapt to new trees and new data.\n\nIdentify only the leaf nodes and have the path.rpart function return the sequence of split conditions that lead to each one.\nApply a function that does step (1) to all of these leaf nodes. This will create a data frame with each leaf node and the relevant rectangle boundaries for their prediction regions.\n\nWe can take these principles and turn them into code. But first, just a little bit more housekeeping.\n\n\n2.2.2 More Housekeeping\nIf all of this felt a little fragmented – fear not! Below, is the entire function written uninterrupted.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  }
]